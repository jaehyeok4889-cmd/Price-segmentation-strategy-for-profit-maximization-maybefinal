# %% 1. Imports & ê¸°ë³¸ ì„¤ì •

import numpy as np
import pandas as pd
from pathlib import Path
import os
import time

import matplotlib.pyplot as plt

# Matplotlib ë°±ì—”ë“œ ì„¤ì • (ì„œë²„/CI í™˜ê²½ í˜¸í™˜ì„±ì„ ìœ„í•´)
# plt.switch_backend('Agg') 

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    roc_auc_score, accuracy_score, precision_recall_fscore_support,
    mean_absolute_error, mean_squared_error
)
from sklearn.isotonic import IsotonicRegression

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# tqdmì€ Colab/Jupyterì—ì„œë§Œ ì œëŒ€ë¡œ ì‘ë™í•˜ë©°, ë¡œì»¬ì—ì„œëŠ” ë‹¨ìˆœ ì¶œë ¥ìœ¼ë¡œ ëŒ€ì²´ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
try:
    from tqdm.auto import tqdm
except ImportError:
    def tqdm(iterable, *args, **kwargs):
        return iterable
    print("Warning: tqdm not installed, using standard iteration.")


# ì¬í˜„ì„±
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# ê°€ê²© & í”¼ì²˜ ì„¤ì •
BASE_PRICE = 9000.0
Feat_Col = ["multi_purchase_count", "total_purchase_last_month", "cart_abandon_count"]

# DNN í•˜ì´í¼íŒŒë¼ë¯¸í„°
EPOCHS = 100
BATCH_SIZE = 32
LEARNING_RATE = 1e-3
VAL_SPLIT = 0.2
TEST_SIZE = 0.20

# %% 2. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬ 

# ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì • (GitHub ì €ì¥ì†Œì— 'input_data.xlsx'ê°€ ìˆë‹¤ê³  ê°€ì •)
data_file_name = "input_data.xlsx"
print(f"Loading data from: {data_file_name}")

try:
    # ì—‘ì…€ ì½ê¸°
    df = pd.read_excel(data_file_name, sheet_name="Sheet1")
except FileNotFoundError:
    print(f"Error: The file '{data_file_name}' was not found.")
    print("Please ensure 'input_data.xlsx' is in the same directory.")
    exit()

df = df.reset_index(drop=True)
df["customer_id"] = df.index.astype(int)

print("Loaded shape:", df.shape)

# ê°€ê²© ì¡°ì •ìš© ë¶„í¬ (0 ì œì™¸)
multi_vals = df.loc[df["multi_purchase_count"] > 0, "multi_purchase_count"]
total_vals = df.loc[df["total_purchase_last_month"] > 0, "total_purchase_last_month"]
abandon_vals = df.loc[df["cart_abandon_count"] > 0, "cart_abandon_count"]


def scale_adjust(value, series, min_adj=-500, max_adj=500):
    """ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ì„ í˜• ë³´ì •"""
    rank = (series <= value).mean()
    return min_adj + (max_adj - min_adj) * rank


def abandon_adjust(value, series, min_adj=-500, max_adj=-100):
    """ë§ì„¤ì„(ì¹´íŠ¸ ì´íƒˆ) ë³´ì •: ê°’ì´ í´ìˆ˜ë¡ ë” í° í• ì¸(ë” ìŒìˆ˜ ìª½)"""
    rank = (series <= value).mean()
    return min_adj + (max_adj - min_adj) * rank


# ì†Œë¹„ìë³„ ë§ì¶¤ ê°€ê²© ê³„ì‚°
adjusted_prices = []

for _, row in df.iterrows():
    multi_val = row["multi_purchase_count"]
    total_val = row["total_purchase_last_month"]
    abandon_val = row["cart_abandon_count"]

    # ê°’ì´ ì—†ì„ ê²½ìš° ë¹ˆ Seriesì— ëŒ€í•œ ì²˜ë¦¬ (avoiding division by zero)
    if multi_vals.empty:
        multi_adj = 0
    else:
        multi_adj = scale_adjust(multi_val, multi_vals)
    
    if total_vals.empty:
        total_adj = 0
    else:
        total_adj = scale_adjust(total_val, total_vals)

    if abandon_val > 0 and not abandon_vals.empty:
        abandon_adj_val = abandon_adjust(abandon_val, abandon_vals)
    else:
        abandon_adj_val = 0.0

    final_price = BASE_PRICE + multi_adj + total_adj + abandon_adj_val

    # ì™„ì „ ì‹ ê·œ(ëª¨ë“  ê°’ 0)ì´ë©´ ê¸°ë³¸ê°€
    if multi_val == 0 and total_val == 0 and abandon_val == 0:
        final_price = BASE_PRICE

    adjusted_prices.append(round(final_price, -2))  # 100ì› ë‹¨ìœ„ ë°˜ì˜¬ë¦¼

df["adjusted_price"] = adjusted_prices

# %% 3. Synthetic label & offered_price ìƒì„± (ìƒëµ ì—†ìŒ)

rng = np.random.default_rng(RANDOM_SEED)

lp_multi = np.log1p(df["multi_purchase_count"].astype(float).values)
lp_total = np.log1p(df["total_purchase_last_month"].astype(float).values)
lp_abnd  = np.log1p(df["cart_abandon_count"].astype(float).values)

sens = 1.0 + 0.15 * (lp_abnd - lp_abnd.mean())
hist_noise = rng.normal(loc=0.0, scale=400.0, size=len(df))

offered_price = (BASE_PRICE * sens + hist_noise).clip(BASE_PRICE * 0.6, BASE_PRICE * 1.5)
df["synthetic_historical_price"] = offered_price

w0 = -0.2
w_total = 0.9
w_multi = 0.8
w_abnd  = 1.0
w_price = 2.0

x_price = (offered_price - BASE_PRICE) / BASE_PRICE
latent = w0 + w_total * lp_total + w_multi * lp_multi - w_abnd * lp_abnd - w_price * x_price
prob   = 1 / (1 + np.exp(-latent))

y = rng.binomial(n=1, p=prob, size=len(df))

df["synthetic_conversion_prob"] = prob
print("Positive rate (mean of y):", np.mean(y).round(4))

# %% 4. í”¼ì²˜ êµ¬ì„± & ìŠ¤ì¼€ì¼ë§ (ìƒëµ ì—†ìŒ)

X_feats = df[Feat_Col].astype(float).copy()
X_feats["offered_price"] = offered_price

for col in Feat_Col:
    X_feats[col] = np.log1p(X_feats[col])

scaler = StandardScaler()
X = scaler.fit_transform(X_feats.values)

feature_names = list(X_feats.columns)
print("Feature order:", feature_names)

# %% 5. DNN ëª¨ë¸ ì •ì˜ (ìƒëµ ì—†ìŒ)

def build_model(input_dim: int) -> keras.Model:
    inp = keras.Input(shape=(input_dim,), name="features")
    x = inp
    for units in [32, 16]:
        x = layers.Dense(units, activation="relu")(x)
        x = layers.Dropout(0.1)(x)
    out = layers.Dense(1, activation="sigmoid", name="purchase_prob")(x)

    model = keras.Model(inputs=inp, outputs=out)
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss="binary_crossentropy",
        metrics=["accuracy", keras.metrics.AUC(name="auc")],
    )
    return model

# %% 6. Train / Val / Test split + í•™ìŠµ & ê²€ì¦ (ê²°ê³¼ ì¶œë ¥ ìœ ì§€)

all_idx = np.arange(X.shape[0])

X_trainval, X_test, y_trainval, y_test, idx_trainval, idx_test = train_test_split(
    X, y, all_idx,
    test_size=TEST_SIZE,
    random_state=RANDOM_SEED,
    stratify=y
)

X_train, X_val, y_train, y_val, idx_train, idx_val = train_test_split(
    X_trainval, y_trainval, idx_trainval,
    test_size=VAL_SPLIT,
    random_state=RANDOM_SEED,
    stratify=y_trainval
)

print(">> Shapes")
print(f"  train: {X_train.shape}  val: {X_val.shape}  test: {X_test.shape}")

model = build_model(X.shape[1])

callbacks = [
    keras.callbacks.EarlyStopping(
        patience=10, restore_best_weights=True,
        monitor="val_auc", mode="max"
    )
]

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=callbacks,
    verbose=0
)

# Test metrics
test_prob = model.predict(X_test, verbose=0).ravel()
test_pred = (test_prob >= 0.5).astype(int)

test_auc = roc_auc_score(y_test, test_prob)
test_acc = accuracy_score(y_test, test_pred)
test_prec, test_rec, test_f1, _ = precision_recall_fscore_support(
    y_test, test_pred, average="binary", zero_division=0
)

mse   = mean_squared_error(y_test, test_prob)
rmse  = float(np.sqrt(mse))
mae   = mean_absolute_error(y_test, test_prob)
brier = mse

print("\n== Test metrics ==")
print(f"AUC:  {test_auc:.4f}  ACC: {test_acc:.4f}  PREC: {test_prec:.4f}  REC: {test_rec:.4f}  F1: {test_f1:.4f}")
print(f"MAE(prob vs label):  {mae:.4f}")
print(f"RMSE(prob vs label): {rmse:.4f}")
print(f"Brier score:         {brier:.4f}")

# %% 7. í—¬í¼ í•¨ìˆ˜ + ì•„í‹°íŒ©íŠ¸ ì €ì¥ 

def predict_probability(customer_id: int, offered_price_input: float) -> float:
    row = df.loc[df["customer_id"] == customer_id].iloc[0]
    feats = [np.log1p(float(row[c])) for c in Feat_Col]
    feats.append(float(offered_price_input))
    feats_scaled = scaler.transform([feats])
    prob = float(model.predict(feats_scaled, verbose=0).ravel()[0])
    return prob

out_dir = Path(".")
if not out_dir.exists():
    out_dir.mkdir(parents=True, exist_ok=True)

model_path = out_dir / "price_propensity_model.h5"
model.save(model_path)
print("Saved:", model_path.resolve())

# %% 8. Price-Demand Curve Panel & Summary CSV ìƒì„± 

BUY_THRESHOLD = 0.75
LOW_RATIO, HIGH_RATIO, STEP_WON = 0.6, 1.5, 100

customer_ids = df["customer_id"].astype(int).tolist()

grid = np.arange(
    max(0, BASE_PRICE * LOW_RATIO),
    BASE_PRICE * HIGH_RATIO + STEP_WON,
    STEP_WON,
    dtype=float
)
G = len(grid)

_fixed = np.zeros((len(customer_ids), len(Feat_Col)), dtype=float)
for j, cid in enumerate(tqdm(customer_ids, desc="Caching features", dynamic_ncols=True, mininterval=0.1)):
    r = df.loc[df["customer_id"] == cid].iloc[0]
    _fixed[j, :] = np.log1p(r[Feat_Col].astype(float).values)


def batch_predict_one_customer(fixed_feats_row: np.ndarray, prices: np.ndarray) -> np.ndarray:
    X = np.column_stack([
        np.repeat(fixed_feats_row.reshape(1, -1), len(prices), axis=0),
        prices.astype(float).reshape(-1, 1)
    ])
    Xs = scaler.transform(X)
    return model.predict(Xs, verbose=0).ravel()


def elasticity_np(price: np.ndarray, q: np.ndarray, eps: float = 1e-9) -> np.ndarray:
    dQ = np.gradient(q)
    dP = np.gradient(price)
    P = np.maximum(eps, price)
    Q = np.maximum(eps, q)
    return (dQ / np.maximum(eps, dP)) * (P / Q)


rows = []
for idx, cid in enumerate(tqdm(customer_ids, desc="Building panel (fast)", dynamic_ncols=True, mininterval=0.1)):
    probs = batch_predict_one_customer(_fixed[idx], grid)
    decision = (probs >= BUY_THRESHOLD).astype(int)
    rev_prob = grid * probs
    elast    = elasticity_np(grid, probs)

    for i in range(G):
        rows.append({
            "customer_id": cid,
            "price": float(grid[i]),
            "prob": float(probs[i]),
            "buy_rule_075": int(decision[i]),
            "revenue_prob": float(rev_prob[i]),
            "elasticity_prob": float(elast[i]),
        })

panel = pd.DataFrame(rows)
panel.to_csv("all_customers_curve.csv", index=False)
print("âœ“ Saved: all_customers_curve.csv")

# %% 9. per-customer summary ì—‘ì…€ + íƒ„ë ¥ì„± ì°¨íŠ¸ 

provided_price_col = "adjusted_price"

def point_elasticity(customer_id: int, price: float, h: float = 100.0) -> float:
    p0, p1 = price - h, price + h
    q0 = float(predict_probability(customer_id, p0))
    q  = float(predict_probability(customer_id, price))
    q1 = float(predict_probability(customer_id, p1))
    dQ_dP = (q1 - q0) / max(1e-9, (p1 - p0))
    if q <= 1e-9:
        return 0.0
    return dQ_dP * (price / q)


BUY_THRESHOLD = 0.75
rows_summary = []

for r in tqdm(df.itertuples(index=False), total=len(df), desc="build summary", dynamic_ncols=True, mininterval=0.1):
    cid = int(getattr(r, "customer_id"))
    price = float(getattr(r, provided_price_col))
    prob  = float(predict_probability(cid, price))
    decision = int(prob >= BUY_THRESHOLD)
    elast = float(point_elasticity(cid, price, h=100.0))

    rows_summary.append({
        "customer_id": cid,
        "multi_purchase_count": int(getattr(r, "multi_purchase_count")),
        "total_purchase_last_month": int(getattr(r, "total_purchase_last_month")),
        "cart_abandon_count": int(getattr(r, "cart_abandon_count")),
        "provided_price": int(round(price)),
        "purchase_probability": round(prob, 4),
        "will_buy_075": decision,
        "elasticity": round(elast, 4),
        "expected_revenue_prob": round(price * prob, 2),
    })

summary = pd.DataFrame(rows_summary)

out_path = "customer_price_elasticity_report.xlsx"
try:
    with pd.ExcelWriter(out_path, engine="openpyxl") as writer:
        cols = ["customer_id", "multi_purchase_count", "total_purchase_last_month", "cart_abandon_count",
                "provided_price", "purchase_probability", "will_buy_075", "elasticity", "expected_revenue_prob"]
        summary[cols].to_excel(writer, index=False, sheet_name="summary")
    print(f"Saved: {out_path}")
except ImportError:
    print("Warning: openpyxl not installed. Saving summary as CSV instead.")
    summary.to_csv("customer_price_elasticity_report.csv", index=False)
    print("Saved: customer_price_elasticity_report.csv")


def save_chart(fig_name: str, title: str, xlabel: str, ylabel: str, data, plot_type='hist'):
    plt.figure(figsize=(7, 5))
    if plot_type == 'hist':
        plt.hist(data, bins=30)
    elif plot_type == 'scatter':
        plt.scatter(data[0], data[1], s=12)
    elif plot_type == 'line':
        plt.plot(data[0], data[1])

    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(fig_name)
    plt.close()

elvals = summary["elasticity"].replace([np.inf, -np.inf], np.nan).dropna().values
if len(elvals) > 0:
    save_chart("chart_elasticity_distribution.png", "Elasticity distribution (at provided price)", "Elasticity (E)", "Count", elvals)

save_chart("chart_price_vs_prob.png", "Provided price vs. purchase probability", "Provided price", "Purchase probability", 
           [summary["provided_price"].values, summary["purchase_probability"].values], plot_type='scatter')

save_chart("chart_price_vs_elasticity.png", "Provided price vs. elasticity", "Provided price", "Elasticity (E)", 
           [summary["provided_price"].values, summary["elasticity"].values], plot_type='scatter')


# %% 10. Aggregate elasticity curve + CSV

def batch_predict_one_customer_agg(row, prices):
    fx = [np.log1p(float(row[c])) for c in Feat_Col]
    X = np.column_stack([
        np.repeat(np.array(fx)[None, :], len(prices), axis=0),
        prices.astype(float).reshape(-1, 1)
    ])
    Xs = scaler.transform(X)
    return model.predict(Xs, verbose=0).ravel()

sum_probs = np.zeros(G, dtype=float)
sum_rev   = np.zeros(G, dtype=float)

for cid in tqdm(customer_ids, total=len(customer_ids), desc="Aggregate probs", dynamic_ncols=True, mininterval=0.1):
    r = df.loc[df["customer_id"] == cid].iloc[0]
    probs = batch_predict_one_customer_agg(r, grid)
    sum_probs += probs
    sum_rev   += grid * probs

N = len(customer_ids)
Q_mean = sum_probs / max(1, N)
Q_tot  = sum_probs
Rev_tot = sum_rev

dQ = np.gradient(Q_mean)
dP = np.gradient(grid)
P  = np.maximum(1e-9, grid)
Q  = np.maximum(1e-9, Q_mean)
E  = (dQ / np.maximum(1e-9, dP)) * (P / Q)

agg_curve = pd.DataFrame({
    "price": grid,
    "Q_mean": Q_mean,
    "Q_total": Q_tot,
    "elasticity_meanQ": E,
    "revenue_total": Rev_tot
})

save_chart("chart_agg_demand.png", "Aggregate demand (mean probability across customers)", "Price", "Q_mean", 
           [agg_curve["price"].values, agg_curve["Q_mean"].values], plot_type='line')
save_chart("chart_agg_elasticity.png", "Aggregate price elasticity (based on mean Q)", "Price", "Elasticity (E)", 
           [agg_curve["price"].values, agg_curve["elasticity_meanQ"].values], plot_type='line')
save_chart("chart_agg_revenue.png", "Total expected revenue across customers", "Price", "Revenue (total)", 
           [agg_curve["price"].values, agg_curve["revenue_total"].values], plot_type='line')


agg_curve.to_csv("aggregate_elasticity_curve.csv", index=False)
print("Saved: aggregate_elasticity_curve.csv")

# %% 11. Conservative pricing + 3-tier ë©”ë‰´ (ê²°ê³¼ ì¶œë ¥ ìœ ì§€)

unit_cost           = 6000.0
PRICE_MIN_FACTOR    = 0.60
PRICE_MAX_FACTOR    = 1.05
GRID_STEP           = 100.0
MIN_PROB            = 0.02
PROB_CAP            = 0.85
SHRINK_LAM          = 0.35
LCB_Z               = 1.64
SLOPE_ALPHA         = 2.0
MENU_K              = 3
PRED_BATCH          = 16384

low  = max(0.0, BASE_PRICE * PRICE_MIN_FACTOR)
high = BASE_PRICE * PRICE_MAX_FACTOR
grid_clip = np.arange(low, high + GRID_STEP, GRID_STEP, dtype=float)
grid_clip = np.asarray(sorted(np.unique(grid_clip)))
G = grid_clip.size

val_raw = model.predict(X_val, verbose=0).ravel()
iso = IsotonicRegression(out_of_bounds="clip").fit(val_raw, y_val)

N = len(df)
F = np.column_stack([
    np.log1p(df[Feat_Col[0]].astype(float).values),
    np.log1p(df[Feat_Col[1]].astype(float).values),
    np.log1p(df[Feat_Col[2]].astype(float).values),
]).astype(float)


def predict_prob_matrix(F, grid_clip, batch=PRED_BATCH):
    N, d = F.shape
    G = grid_clip.size
    Pcol = np.repeat(grid_clip.reshape(1, G), N, axis=0).reshape(-1, 1)
    Fbig = np.repeat(F, G, axis=0)
    Xbig = np.concatenate([Fbig, Pcol], axis=1)
    Xs   = scaler.transform(Xbig)

    raw = np.empty(Xs.shape[0], dtype=float)
    for s in tqdm(range(0, Xs.shape[0], batch),
                  desc="Predict (batched)", dynamic_ncols=True, mininterval=0.1):
        e = min(s + batch, Xs.shape[0])
        raw[s:e] = model.predict(Xs[s:e], verbose=0).ravel()

    prob = iso.predict(raw)
    p_ref = float(np.mean(y_val))
    prob = (1 - SHRINK_LAM) * prob + SHRINK_LAM * p_ref
    prob = np.clip(prob, 1e-6, PROB_CAP)
    return prob.reshape(N, G)


P_raw = predict_prob_matrix(F, grid_clip) 

def _logit(p):
    p = np.clip(p, 1e-6, 1-1e-6)
    return np.log(p / (1 - p))

def _sigm(z):
    return 1 / (1 + np.exp(-z))

price_tilt = (grid_clip - BASE_PRICE) / max(1e-9, BASE_PRICE)
Z = _logit(P_raw) - SLOPE_ALPHA * price_tilt.reshape(1, -1)
P_raw = np.clip(_sigm(Z), 1e-6, PROB_CAP)

order = np.argsort(grid_clip)
inv   = np.argsort(order)
P_sorted = P_raw[:, order]
P_sorted = np.minimum.accumulate(P_sorted, axis=1)
P = np.clip(P_sorted[:, inv], MIN_PROB, PROB_CAP)

Var   = np.maximum(P * (1 - P), 1e-9)
P_lcb = np.clip(P - LCB_Z * np.sqrt(Var), 1e-6, PROB_CAP)
Profit = (grid_clip.reshape(1, -1) - unit_cost) * P_lcb 

best_idx    = np.argmax(Profit, axis=1)
best_price  = grid_clip[best_idx]
best_profit = Profit[np.arange(N), best_idx]

# Elasticity (at base price)
base_idx = int(np.argmin(np.abs(grid_clip - BASE_PRICE)))
base_idx = int(np.clip(base_idx, 1, G-2))
im1, ip1 = base_idx - 1, base_idx + 1
dQ = P_raw[:, ip1] - P_raw[:, im1]
dP = (grid_clip[ip1] - grid_clip[im1]) + 1e-9
Qb = np.maximum(P_raw[:, base_idx], 1e-9)
Pb = grid_clip[base_idx]
elas = (dQ / dP) * (Pb / Qb)

# absolute elasticity tertile segment
absE = np.abs(elas)
rank = absE.argsort().argsort()
segments = np.zeros(N, dtype=int)

cut1 = N // 3
cut2 = 2 * N // 3
segments[(rank >= cut1) & (rank < cut2)] = 1
segments[rank >= cut2] = 2

for s in (0, 1, 2):
    if np.sum(segments == s) == 0:
        k = np.argmin(np.abs(rank - (cut1 if s == 0 else (cut2 if s == 2 else (cut1+cut2)//2))))
        segments[k] = s

# 3-tier ë©”ë‰´ ê°€ê²©
print("\n3-tier MENU (balanced tertiles) Optimal Price:")
menu_prices = {}
menu_results = []
for seg in range(MENU_K):
    idx = np.where(segments == seg)[0]
    seg_profit_mean = Profit[idx].mean(axis=0)
    j = int(np.argmax(seg_profit_mean))
    mp = int(grid_clip[j])
    mp_prob = float(P[idx, j].mean())
    mp_profit = float(seg_profit_mean[j])
    menu_prices[seg] = mp
    print(f"SEG{seg}  n={idx.size:4}  price={mp:5}  mean_prob={mp_prob:.3f}  mean_profit={mp_profit:.2f}")
    menu_results.append({
        "segment": seg,
        "n_customers": idx.size,
        "menu_price": mp,
        "mean_prob": mp_prob,
        "mean_profit": mp_profit
    })

menu_df = pd.DataFrame(menu_results)
menu_df.to_csv("3_tier_menu_results.csv", index=False)
print("Saved: 3_tier_menu_results.csv")


# BASE vs ë©”ë‰´ uplift
base_idx = int(np.argmin(np.abs(grid_clip - BASE_PRICE)))
base_mean = float(Profit[:, base_idx].mean())
menu_vec = np.zeros(N, dtype=float)

for seg in range(MENU_K):
    idx = np.where(segments == seg)[0]
    j = int(np.where(grid_clip == menu_prices[seg])[0][0])
    menu_vec[idx] = Profit[idx, j]

menu_mean = float(menu_vec.mean())
uplift = (menu_mean - base_mean) / max(1e-9, base_mean)

print(f"\nTotal expected profit uplift vs BASE({int(BASE_PRICE)}): {uplift*100:.2f}%")

<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI ê¸°ë°˜ ê°œì¸í™” ê°€ê²© ì „ëµ ë¶„ì„ ëŒ€ì‹œë³´ë“œ</title>
    <style>
        body { font-family: 'Arial', sans-serif; margin: 0; padding: 0; background-color: #f4f7f6; }
        .container { width: 90%; max-width: 1400px; margin: 20px auto; background-color: #fff; padding: 30px; border-radius: 12px; box-shadow: 0 6px 20px rgba(0,0,0,0.15); }
        h1 { color: #2c3e50; text-align: center; margin-bottom: 30px; border-bottom: 3px solid #3498db; padding-bottom: 15px; }
        h2 { color: #34495e; margin-top: 40px; padding-bottom: 8px; border-left: 5px solid #1abc9c; padding-left: 10px; }
        h3 { color: #16a085; margin-top: 25px; }
        .data-table { width: 100%; border-collapse: collapse; margin-top: 20px; font-size: 0.9em; }
        .data-table th, .data-table td { border: 1px solid #ddd; padding: 12px 8px; text-align: center; }
        .data-table th { background-color: #3498db; color: white; font-weight: bold; }
        .data-table tr:nth-child(even) { background-color: #f8f8f8; }
        .grid-3 { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin-top: 20px; }
        .grid-2 { display: grid; grid-template-columns: repeat(auto-fit, minmax(350px, 1fr)); gap: 20px; margin-top: 20px; }
        .graph-container img { width: 100%; height: auto; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }
        .graph-container h4 { text-align: center; color: #7f8c8d; }
        .metric-box { background-color: #ecf0f1; padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 2px 4px rgba(0,0,0,0.05); }
        .highlight { font-size: 2.5em; font-weight: bold; margin-top: 5px; display: block; }
        .uplift-highlight { color: #e74c3c; background-color: #fbeee6; padding: 10px; border-radius: 6px; }
        .auc-highlight { color: #2ecc71; }
        .error-highlight { color: #f39c12; }
    </style>
</head>
<body>

<div class="container">
    <h1>ğŸš€ AI ê¸°ë°˜ ê°œì¸í™” ê°€ê²© ì „ëµ ë¶„ì„ ëŒ€ì‹œë³´ë“œ</h1>
    
    <section>
        <h2>1. ğŸ“‹ ê³ ê°ë³„ ê°€ê²© ì •ì±… ìš”ì•½ (Customer Price & Elasticity Report)</h2>
        <p>ì „ì²´ ë¶„ì„ ê²°ê³¼ëŠ” **`customer_price_elasticity_report.xlsx`** íŒŒì¼ì„ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤. ì•„ë˜ëŠ” ìƒìœ„ 5ê°œ ê³ ê°ì˜ ìƒ˜í”Œì…ë‹ˆë‹¤.</p>
        <table class="data-table main-summary">
            <thead>
                <tr>
                    <th>customer_id</th>
                    <th>multi_purchase_count</th>
                    <th>total_purchase_last_month</th>
                    <th>cart_abandon_count</th>
                    <th>**provided_price**</th>
                    <th>**purchase_probability**</th>
                    <th>will_buy_075</th>
                    <th>**elasticity**</th>
                    <th>expected_revenue_prob</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>0</td><td>0</td><td>0</td><td>0</td><td>9000</td><td>0.4079</td><td>0</td><td>-0.6698</td><td>3671.10</td></tr>
                <tr><td>1</td><td>3</td><td>12</td><td>0</td><td>8800</td><td>0.5481</td><td>0</td><td>-0.9921</td><td>4823.28</td></tr>
                <tr><td>2</td><td>1</td><td>2</td><td>0</td><td>8500</td><td>0.5113</td><td>0</td><td>-1.1578</td><td>4346.05</td></tr>
                <tr><td>3</td><td>2</td><td>10</td><td>1</td><td>9300</td><td>0.4633</td><td>0</td><td>-0.8355</td><td>4308.70</td></tr>
                <tr><td>4</td><td>0</td><td>0</td><td>0</td><td>9000</td><td>0.4079</td><td>0</td><td>-0.6698</td><td>3671.10</td></tr>
            </tbody>
        </table>
    </section>

    <hr>
    
    <section>
        <h2>2. ğŸ“Š ê°€ê²© ìˆ˜ìš” ë¶„ì„ ë° ì§‘ê³„ ì‹œê°í™”</h2>
        
        <h3>ğŸ“ˆ ì§‘ê³„ ìˆ˜ìš” ë° ìˆ˜ìµ ê³¡ì„ </h3>
        <div class="grid-3">
            <div class="graph-container">
                <h4>ì´ ìˆ˜ìš” ê³¡ì„  (Q_mean)</h4>
                <img src="chart_agg_demand.png" alt="Aggregate Demand Curve">
            </div>
            <div class="graph-container">
                <h4>ì´ ì˜ˆìƒ ìˆ˜ìµ ê³¡ì„  (Revenue)</h4>
                <img src="chart_agg_revenue.png" alt="Total Expected Revenue">
            </div>
            <div class="graph-container">
                <h4>ì§‘ê³„ íƒ„ë ¥ì„± ê³¡ì„  (E)</h4>
                <img src="chart_agg_elasticity.png" alt="Aggregate Elasticity Curve">
            </div>
        </div>

        <h3>ğŸ“Œ ê³ ê°ë³„ í˜„í™© ì‚°ì ë„</h3>
        <div class="grid-2">
             <div class="graph-container">
                <h4>ì œì‹œ ê°€ê²© vs. êµ¬ë§¤ í™•ë¥ </h4>
                <img src="chart_price_vs_prob.png" alt="Price vs Prob Scatter">
            </div>
             <div class="graph-container">
                <h4>ì œì‹œ ê°€ê²© vs. íƒ„ë ¥ì„±</h4>
                <img src="chart_price_vs_elasticity.png" alt="Price vs Elasticity Scatter">
            </div>
        </div>
    </section>

    <hr>

    <section>
        <h2>3. ğŸ¯ íƒ„ë ¥ì„± ê¸°ë°˜ 3-Tier ë©”ë‰´ ê°€ê²© ì „ëµ ë¶„ì„</h2>
        <p>ê³ ê°ì˜ íƒ„ë ¥ì„±(ê°€ê²© ë¯¼ê°ë„)ì— ë”°ë¥¸ 3ë‹¨ê³„ ë©”ë‰´ ê°€ê²© ìµœì í™” ê²°ê³¼ì…ë‹ˆë‹¤.</p>

        <h3>ğŸ’° 3-Tier ë©”ë‰´ ê°€ê²© ë° ì´ìœ¤</h3>
        <table class="data-table">
            <thead>
                <tr><th>ì„¸ê·¸ë¨¼íŠ¸</th><th>ì„¤ëª…</th><th>ê³ ê° ìˆ˜ (n)</th><th>**ìµœì  ë©”ë‰´ ê°€ê²©**</th><th>í‰ê·  êµ¬ë§¤ í™•ë¥ </th><th>í‰ê·  ì˜ˆìƒ ì´ìœ¤ (LCB)</th></tr>
            </thead>
            <tbody>
                <tr><td>SEG 0</td><td>**ì €íƒ„ë ¥** (ê°€ê²© ë¹„ë¯¼ê°)</td><td>333</td><td>**9400ì›**</td><td>0.448</td><td>1508.82ì›</td></tr>
                <tr><td>SEG 1</td><td>**ì¤‘íƒ„ë ¥** (ì¤‘ê°„ ë¯¼ê°)</td><td>334</td><td>**9300ì›**</td><td>0.465</td><td>1559.87ì›</td></tr>
                <tr><td>SEG 2</td><td>**ê³ íƒ„ë ¥** (ê°€ê²© ë¯¼ê°)</td><td>333</td><td>**8200ì›**</td><td>0.510</td><td>1112.55ì›</td></tr>
            </tbody>
        </table>

        <div class="metric-box" style="margin-top: 30px;">
            <h3>ğŸ“ˆ BASE ê°€ê²© ëŒ€ë¹„ ì´ ì˜ˆìƒ ì´ìœ¤ í–¥ìƒë¥  (Uplift)</h3>
            <p>ê¸°ë³¸ ê°€ê²©(9000ì›) ì •ì±… ëŒ€ë¹„ 3-Tier ì „ëµ ì ìš© ì‹œ ì˜ˆìƒ ì´ìœ¤ í–¥ìƒ:</p>
            <span class="highlight uplift-highlight">+17.33%</span>
        </div>
    </section>

    <hr>
    
    <section>
        <h2>4. âœ… êµ¬ë§¤ í™•ë¥  ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ (Test Metrics)</h2>
        
        <div class="grid-3">
            <div class="metric-box">
                <h4>AUC (ë¶„ë¥˜ ì„±ëŠ¥)</h4>
                <span class="highlight auc-highlight">0.8521</span>
            </div>
            <div class="metric-box">
                <h4>Brier Score (í™•ë¥  ì •í™•ë„)</h4>
                <span class="highlight error-highlight">0.1876</span>
            </div>
            <div class="metric-box">
                <h4>RMSE (ì˜ˆì¸¡ ì˜¤ì°¨)</h4>
                <span class="highlight error-highlight">0.4331</span>
            </div>
        </div>
        
        <table class="data-table" style="margin-top: 20px;">
            <thead>
                <tr><th>Accuracy</th><th>Precision</th><th>Recall</th><th>F1 Score</th><th>MAE</th></tr>
            </thead>
            <tbody>
                <tr><td>0.7580</td><td>0.7495</td><td>0.8140</td><td>0.7812</td><td>0.3475</td></tr>
            </tbody>
        </table>
    </section>

</div>

</body>
</html>
