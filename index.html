# %% 1. Imports & 기본 설정


import numpy as np
import pandas as pd
from google.colab import files
from pathlib import Path


import matplotlib.pyplot as plt


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    roc_auc_score, accuracy_score, precision_recall_fscore_support,
    mean_absolute_error, mean_squared_error
)
from sklearn.isotonic import IsotonicRegression


import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


from tqdm.auto import tqdm


# 재현성
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)


# 가격 & 피처 설정
BASE_PRICE = 9000.0
Feat_Col = ["multi_purchase_count", "total_purchase_last_month", "cart_abandon_count"]


# DNN 하이퍼파라미터
EPOCHS = 100
BATCH_SIZE = 32
LEARNING_RATE = 1e-3
VAL_SPLIT = 0.2
TEST_SIZE = 0.20


# %% 2. 데이터 업로드 & 기본 전처리


uploaded = files.upload()
fname = list(uploaded.keys())[0]


# 엑셀 읽기
df = pd.read_excel(fname, sheet_name="Sheet1")
df = df.reset_index(drop=True)
df["customer_id"] = df.index.astype(int)


print("Loaded shape:", df.shape)
print("Columns:", df.columns.tolist())


# 가격 조정용 분포 (0 제외)
multi_vals = df.loc[df["multi_purchase_count"] > 0, "multi_purchase_count"]
total_vals = df.loc[df["total_purchase_last_month"] > 0, "total_purchase_last_month"]
abandon_vals = df.loc[df["cart_abandon_count"] > 0, "cart_abandon_count"]




def scale_adjust(value, series, min_adj=-500, max_adj=500):
    """분위수 기반 선형 보정"""
    rank = (series <= value).mean()
    return min_adj + (max_adj - min_adj) * rank




def abandon_adjust(value, series, min_adj=-500, max_adj=-100):
    """망설임(카트 이탈) 보정: 값이 클수록 더 큰 할인(더 음수 쪽)"""
    rank = (series <= value).mean()
    return min_adj + (max_adj - min_adj) * rank




# 소비자별 맞춤 가격 계산
adjusted_prices = []


for _, row in df.iterrows():
    multi_val = row["multi_purchase_count"]
    total_val = row["total_purchase_last_month"]
    abandon_val = row["cart_abandon_count"]


    multi_adj = scale_adjust(multi_val, multi_vals)
    total_adj = scale_adjust(total_val, total_vals)


    if abandon_val > 0:
        abandon_adj_val = abandon_adjust(abandon_val, abandon_vals)
    else:
        abandon_adj_val = 0.0


    final_price = BASE_PRICE + multi_adj + total_adj + abandon_adj_val


    # 완전 신규(모든 값 0)이면 기본가
    if multi_val == 0 and total_val == 0 and abandon_val == 0:
        final_price = BASE_PRICE


    adjusted_prices.append(round(final_price, -2))  # 100원 단위 반올림


df["adjusted_price"] = adjusted_prices


# %% 3. Synthetic label & offered_price 생성


rng = np.random.default_rng(RANDOM_SEED)


lp_multi = np.log1p(df["multi_purchase_count"].astype(float).values)
lp_total = np.log1p(df["total_purchase_last_month"].astype(float).values)
lp_abnd  = np.log1p(df["cart_abandon_count"].astype(float).values)


# 과거 히스토리 기반 가격 (대략적 변동)
sens = 1.0 + 0.15 * (lp_abnd - lp_abnd.mean())  # 망설임이 많을수록 살짝 감도 조정
hist_noise = rng.normal(loc=0.0, scale=400.0, size=len(df))


offered_price = (BASE_PRICE * sens + hist_noise).clip(BASE_PRICE * 0.6, BASE_PRICE * 1.5)
df["synthetic_historical_price"] = offered_price


# 잠재 로짓 모델 → 구매 확률
w0 = -0.2
w_total = 0.9
w_multi = 0.8
w_abnd  = 1.0
w_price = 2.0


x_price = (offered_price - BASE_PRICE) / BASE_PRICE
latent = w0 + w_total * lp_total + w_multi * lp_multi - w_abnd * lp_abnd - w_price * x_price
prob   = 1 / (1 + np.exp(-latent))


y = rng.binomial(n=1, p=prob, size=len(df))


df["synthetic_conversion_prob"] = prob
print("Positive rate (mean of y):", np.mean(y).round(4))


# %% 4. 피처 구성 & 스케일링


X_feats = df[Feat_Col].astype(float).copy()
X_feats["offered_price"] = offered_price


# count 계열은 log1p
for col in Feat_Col:
    X_feats[col] = np.log1p(X_feats[col])


scaler = StandardScaler()
X = scaler.fit_transform(X_feats.values)


feature_names = list(X_feats.columns)
print("Feature order:", feature_names)


# %% 5. DNN 모델 정의


def build_model(input_dim: int) -> keras.Model:
    inp = keras.Input(shape=(input_dim,), name="features")
    x = inp
    for units in [32, 16]:
        x = layers.Dense(units, activation="relu")(x)
        x = layers.Dropout(0.1)(x)
    out = layers.Dense(1, activation="sigmoid", name="purchase_prob")(x)


    model = keras.Model(inputs=inp, outputs=out)
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss="binary_crossentropy",
        metrics=["accuracy", keras.metrics.AUC(name="auc")],
    )
    return model


# %% 6. Train / Val / Test split + 학습 & 검증


all_idx = np.arange(X.shape[0])


# 1) test 20% 홀드아웃
X_trainval, X_test, y_trainval, y_test, idx_trainval, idx_test = train_test_split(
    X, y, all_idx,
    test_size=TEST_SIZE,
    random_state=RANDOM_SEED,
    stratify=y
)


# 2) train / val 분리
X_train, X_val, y_train, y_val, idx_train, idx_val = train_test_split(
    X_trainval, y_trainval, idx_trainval,
    test_size=VAL_SPLIT,
    random_state=RANDOM_SEED,
    stratify=y_trainval
)


print(">> Shapes")
print(f"  train: {X_train.shape}  val: {X_val.shape}  test: {X_test.shape}")


# 모델 학습
model = build_model(X.shape[1])


callbacks = [
    keras.callbacks.EarlyStopping(
        patience=10, restore_best_weights=True,
        monitor="val_auc", mode="max"
    )
]


history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=callbacks,
    verbose=0
)


# Validation metrics
val_prob = model.predict(X_val, verbose=0).ravel()
val_pred = (val_prob >= 0.5).astype(int)


val_auc = roc_auc_score(y_val, val_prob)
val_acc = accuracy_score(y_val, val_pred)
val_prec, val_rec, val_f1, _ = precision_recall_fscore_support(
    y_val, val_pred, average="binary", zero_division=0
)


print("\n== Validation metrics ==")
print(f"AUC:  {val_auc:.4f}  ACC: {val_acc:.4f}  PREC: {val_prec:.4f}  REC: {val_rec:.4f}  F1: {val_f1:.4f}")


# Test metrics (prob vs label)
test_prob = model.predict(X_test, verbose=0).ravel()
test_pred = (test_prob >= 0.5).astype(int)


test_auc = roc_auc_score(y_test, test_prob)
test_acc = accuracy_score(y_test, test_pred)
test_prec, test_rec, test_f1, _ = precision_recall_fscore_support(
    y_test, test_pred, average="binary", zero_division=0
)


mse   = mean_squared_error(y_test, test_prob)
rmse  = float(np.sqrt(mse))
mae   = mean_absolute_error(y_test, test_prob)
brier = mse  # Brier score = MSE(prob, label)


print("\n== Test metrics ==")
print(f"AUC:  {test_auc:.4f}  ACC: {test_acc:.4f}  PREC: {test_prec:.4f}  REC: {test_rec:.4f}  F1: {test_f1:.4f}")
print(f"MAE(prob vs label):  {mae:.4f}")
print(f"RMSE(prob vs label): {rmse:.4f}")
print(f"Brier score:         {brier:.4f}")


